# 机器学习中常见面试题与答案（一）

这里罗列机器学习中常见的面试题与我自己整理的答案。

## sigmoid 和 softmax 区别与联系

- sigmoid也叫logistic，输出值在[0,1]之间，作为激活函数会存在梯度消失的问题；可用于二分类问题中；
- softmax是sigmoid在多分类的推广，输出每个类别的概率，加和为1；当类别为2时，与sigmoid等价；
- softmax并不存在学习慢的问题，参考[这里](https://blog.csdn.net/Bixiwen_liu/article/details/52946867)；
- softmax放在最后一层，cost function用cross entropy，[原因](https://www.zhihu.com/question/40403377)

## 常用激活函数比较

[reference](https://www.jianshu.com/p/22d9720dbf1a)

神经网络用激活函数，是为了给网络引入非线性元素，使得网络可以逼近任何非线性函数。    
常见的有sigmoid, tanh, Relu及其变体；

- sigmoid: 在特征相差不是特别大，或者特征比较复杂的时候，效果比较好；还可以用来做二分类；但是：激活函数计算量大，并且容易出现梯度消失的问题；
- tanh: 输出值在[-1, 1]之间，零点对称，实际应用中效果比sigmoid好；在特征相差明显时效果比较好；
- Relu：优点是在SGD时收敛速度很快，不存在梯度消失的问题，但是比较脆弱，很容易die,神经元失活，要注意学习率设置不能过大；
- Relu单侧抑制，具有相对宽阔的兴奋边界，稀疏激活性；


## Dropout作用，防止过拟合的方式







## weight decay原理与作用

## 损失函数表达式

- 最小平方项
- 对数损失函数


## padding 方式


## 各种曲线的含义：ROC，AUC，代价曲线等； ROC和代价曲线的联系


## SVM相关

### SVM硬核表达式

### SMO作用，缺点

### SVM为什么可以逼近任意函数

## 梯度消失，梯度爆炸是什么，怎么解决   

## 神经网络输入图片大小为什么要一样，不一样的怎么办


## 数据归一化方式

## 在训练一开始loss就不变，是怎么回事，怎么排查这个问题


## Batch Normalization


## Resnet 相关

### Resnet有几种shortcut方式


### Resnet的残差结构特点，Desnet...








- l1,l2正则化和区别
- XGBoost和提升树
- relu  
- 梯度消失
- 循环神经网络，为什么好
- 朴素贝叶斯为什么称为“朴素”；以及朴素贝叶斯的公式表达
从Kmeans、KNN，还有一个我忘了，选择一个算法进行详细的描述及应用说明
机器学习有哪些损失函数
对数损失和均方损失有什么区别
逻辑回归能不能用均方损失
机器学习有哪些评价指标
11.解释一下F1-SCORE？
12.解释一下auc roc？
13.介绍一下决策树?
14.python元组和列表的区别
15.讲一下二叉树排序？
16.红黑树了解嘛？
Bagging和boosting的区别
2-讲讲随机森林的原理？主要是随机属性引入加集成。。

3-Linux 复制命令是什么？查询帮助用什么命令？cp --help
7-xgboost和gbdt区别？一个利用牛顿法二阶导数信息，正则项加上了树的复杂度，缺失值样本不同权重处理？一个一阶导啥的。

8-随机森林和gbdt谁主要降低偏差和方差？gbdt，随机森林。

有哪些分类模型
（4）然后问正则化有哪些
（5）问L1和L2为什么一个稀疏一个平滑
（6）问梯度下降算法与牛顿算法的优缺点
（7）问BP的过程，然后链式求导怎么做
（8）问SVM与贝叶斯的优缺点
（9）问KNN如何解决数据维度问题（不会）

lr和svm，比较优缺点